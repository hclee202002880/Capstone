{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "410b7128-7d3b-453f-86d4-4856a1728510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transplanted\n",
      "False    124129\n",
      "True       8972\n",
      "Name: count, dtype: int64\n",
      "[False  True]\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# 1. 데이터 불러오기 - Excel 파일로 수정\n",
    "referrals = pd.read_excel(r\"C:\\Users\\이희창\\Downloads\\opd.xlsx\", engine='openpyxl')\n",
    "# 또는 확장자가 없는 경우: \n",
    "# referrals = pd.read_excel(r\"C:\\Users\\이희창\\Downloads\\opd\", engine='openpyxl')\n",
    "\n",
    "df = referrals\n",
    "print(referrals['transplanted'].value_counts())\n",
    "print(referrals['transplanted'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b809c0-dc2a-46fe-a022-091855411ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PatientID, HospitalID 및 outcome으로 시작하는 변수들을 원본 데이터프레임(df)에서 제거\n",
    "outcome_columns = [col for col in df.columns if col.startswith('outcome_')]\n",
    "columns_to_drop = ['PatientID', 'HospitalID'] + outcome_columns\n",
    "\n",
    "print(\"제거할 변수들:\", columns_to_drop)\n",
    "\n",
    "# 원본 데이터프레임에서 선택된 변수들 제거\n",
    "df = df.drop(columns=columns_to_drop, axis=1)\n",
    "print(f\"변수 제거 후 원본 데이터프레임 크기: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029655f3-5ba8-4451-a516-b5b13d46224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_data(data):\n",
    "  \"\"\"\n",
    "  Returns DataFrame with percent missing data from input data (DataFrame).\n",
    "\n",
    "  Parameters\n",
    "  -----\n",
    "  data (DataFrame): input dataframe\n",
    "\n",
    "  Returns\n",
    "  -----\n",
    "  missing_data (DataFrame): output dataframe with % missing values\n",
    "  \"\"\"\n",
    "\n",
    "  #print(data.isnull().sum()) # uncomment this if you want to see list of counts\n",
    "\n",
    "  # Get percentage of missing values in each column\n",
    "  missing_data_prop={}\n",
    "  for x,y in enumerate(list(data.isnull().sum())):\n",
    "    missing_data_prop[data.columns[x]]=(float(y/data.shape[0])*100) #\"{:.2f}\".format\n",
    "\n",
    "  missing_data=pd.DataFrame(missing_data_prop.items(), columns=['column', 'percent_missing'])\n",
    "  return missing_data\n",
    "\n",
    "missing_data=get_missing_data(df)\n",
    "missing_data\n",
    "\n",
    "df_new = df.copy()\n",
    "def total_values(df,col,list_features,label):\n",
    "  for i in list_features:\n",
    "    #print(col,i)\n",
    "    #Change each column value to the new label based on classification framework\n",
    "    df[col].mask(df[col]==i, label, inplace=True)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d0b27-7058-4097-b453-22d9ab093c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "infections=['Sepsis','Septic Shock','Infectious Disease - Bacterial','Infectious Disease - Viral',\n",
    "            'Infectious Disease - Other, specify','Pneumonia','HIV','Hepatitis','AIDS/HIV']\n",
    "total_values(df_new,'Cause_of_Death_OPO',infections,'Infectious Disease')\n",
    "\n",
    "# Cardio\n",
    "cardio=['CHF','CAR - CHF','AAA or thoracic AA', 'AAA - abdominal aortic aneurysm', 'CAR - cardiomegaly/cardiomyopathy/cardiovascular',\n",
    "        'Pulmonary embolism','PE--Pulmonary Embolism ','Myocardial infarction',\n",
    "        'CAR - MI', 'CAR - probable MI', 'CAR - arrhythmia',\n",
    "        'Arrhythmia','Cardiac - Other, specify']\n",
    "total_values(df_new,'Cause_of_Death_OPO',cardio,'Circulatory Disease')\n",
    "\n",
    "# Respiratory\n",
    "resp=['Anoxia','COPD','RES - COPD', 'Respiratory - Other','Respiratory - Other, specify',\n",
    "      'RES - other', 'RES - pneumonia', 'RES - lung disease', 'RES - asthma',\n",
    "      'RES - aspiration']\n",
    "total_values(df_new,'Cause_of_Death_OPO',resp,'Respiratory Disease')\n",
    "\n",
    "# Newborn/perinatal\n",
    "newborn=['Fetal Demise','Prematurity','Sudden infant death syndrome',\n",
    "         'PED - abuse/shaken baby']\n",
    "total_values(df_new,'Cause_of_Death_OPO',newborn,'Newborn Disease')\n",
    "\n",
    "# Cancers\n",
    "cancers=['Leukemia / Lymphoma','Cancer', 'Cancer - Leukemia/Lymphoma','Cancer/Current or within five years']\n",
    "total_values(df_new,'Cause_of_Death_OPO',cancers,'Cancer')\n",
    "\n",
    "# Neurological\n",
    "neuro=['CVA/Stroke - Cerebro Accident','ICB / ICH', 'Cerebrovascular / Stroke',\n",
    "       'CNS Tumor','SAH','Meningitis','Seizure/Seizure Disorder', 'Aneurysm',\n",
    "       ]\n",
    "total_values(df_new,'Cause_of_Death_OPO',neuro,'Nervous Disease')\n",
    "\n",
    "# Digestive\n",
    "digestive=['GI - necrotic bowel','GI - bleed','GI - bowel perforation','GI - bowel obstruction']\n",
    "total_values(df_new,'Cause_of_Death_OPO',digestive,'Digestive Disease')\n",
    "\n",
    "# Liver\n",
    "liver=['Liver Disease/Failure','ESLD']\n",
    "total_values(df_new,'Cause_of_Death_OPO',liver,'Liver Disease')\n",
    "\n",
    "# Kidney\n",
    "kidney=['ESRD','Kidney/Renal  Disease']\n",
    "total_values(df_new,'Cause_of_Death_OPO',kidney,'Kidney Disease')\n",
    "\n",
    "# Eye\n",
    "eye=['PED - other', 'PED - premature']\n",
    "total_values(df_new,'Cause_of_Death_OPO',eye,'Eye Disease')\n",
    "\n",
    "# Injuries, mostly external\n",
    "injury=['GSW','TR - GSW','Drowning','Head Trauma','Trauma','Overdose',\n",
    "        'Drug Overdose/Probable Drug Abuse','An - other', 'An - asphyixiation',\n",
    "        'An - smoke inhalation','An -  hanging', 'An - drowning',\n",
    "        'TR - MVA', 'TR - other', 'TR - other', 'TR - CHI - Closed Head Injury',\n",
    "        'TR - burns', 'TR - stabbing', 'TR - electrocution','Poisoning',\n",
    "        'Intracranial Hemorrhage','Exsanguination']\n",
    "total_values(df_new,'Cause_of_Death_OPO',injury,'Injury_External Causes')\n",
    "\n",
    "# Multisystem\n",
    "multi=['Multi-system failure', 'MultiSystem Failure']\n",
    "total_values(df_new,'Cause_of_Death_OPO',multi,'Multi-system failure')\n",
    "\n",
    "# Other\n",
    "other=['Other','Other, specify']\n",
    "total_values(df_new,'Cause_of_Death_OPO',other,'Other')\n",
    "\n",
    "#Cluster categories: cause of death UNOS\n",
    "\n",
    "infections=['Sepsis','Infectious Disease - Bacterial','Infectious Disease - Viral','Infectious Disease - Other, specify','Pneumonia','HIV','Hepatitis']\n",
    "total_values(df_new,'Cause_of_Death_UNOS',infections,'Infectious Disease')\n",
    "\n",
    "cardio=['CHF','AAA or thoracic AA', 'Pulmonary embolism','Myocardial infarction','Arrhythmia','Cardiac - Other, specify']\n",
    "total_values(df_new,'Cause_of_Death_UNOS',cardio,'Circulatory Disease')\n",
    "\n",
    "resp=['Anoxia','COPD','Respiratory - Other','Respiratory - Other, specify']\n",
    "total_values(df_new,'Cause_of_Death_UNOS',resp,'Respiratory Disease')\n",
    "\n",
    "newborn=['Fetal Demise','Prematurity','Sudden infant death syndrome']\n",
    "total_values(df_new,'Cause_of_Death_UNOS',newborn,'Newborn Disease')\n",
    "\n",
    "cancers=['Leukemia / Lymphoma','Cancer']\n",
    "total_values(df_new,'Cause_of_Death_UNOS',cancers,'Cancer')\n",
    "\n",
    "neuro=['CVA/Stroke','ICB / ICH', 'Cerebrovascular / Stroke', 'CNS Tumor','SAH']\n",
    "total_values(df_new,'Cause_of_Death_UNOS',neuro,'Nervous Disease')\n",
    "\n",
    "injury=['GSW','Drowning','Head Trauma','Trauma','Overdose',\n",
    "        'Exsanguination']\n",
    "total_values(df_new,'Cause_of_Death_UNOS',injury,'Injury_External Causes')\n",
    "\n",
    "other=['Other','Other, specify']\n",
    "total_values(df_new,'Cause_of_Death_UNOS',other,'Other')\n",
    "\n",
    "# Replace names to keep consistent with OPO category change\n",
    "df_new['Cause_of_Death_UNOS'].replace('ESRD', 'Kidney Disease', inplace=True)\n",
    "df_new['Cause_of_Death_UNOS'].replace('ESLD', 'Liver Disease', inplace=True)\n",
    "\n",
    "# Cluster categories: mechanism of death\n",
    "\n",
    "# Taking only natural causes\n",
    "natural_causes=['Natural Causes','Death from Natural Causes']\n",
    "total_values(df_new,'Mechanism_of_Death',natural_causes,'Natural Causes')\n",
    "\n",
    "# Taking only injuries and external causes: blunt injury, drug intoxication, gunshot wound, asphyxiation, drowning, stab, electrical\n",
    "injury_external=['Blunt Injury','Drug Intoxication','Gun Shot Wound','Asphyxiation','Drug / Intoxication',\n",
    "                 'Drowning','Gunshot Wound','Stab','Electrical']\n",
    "total_values(df_new,'Mechanism_of_Death',injury_external,'Injury_External Causes')\n",
    "\n",
    "# Taking only nervous system related disorders: stroke, seizure\n",
    "nervous_diseases=['ICH/Stroke','Intracranial Hemmorrhage / Stroke','Seizure']\n",
    "total_values(df_new,'Mechanism_of_Death',nervous_diseases,'Nervous Disease')\n",
    "\n",
    "# None of the above\n",
    "nofa=['None of the Above','None of the above']\n",
    "total_values(df_new,'Mechanism_of_Death',nofa,'Other')\n",
    "\n",
    "# Cluster categories: Circumstances of Death\n",
    "\n",
    "# Taking only natural causes\n",
    "natural_causes=['Natural Causes','Death from Natural Causes']\n",
    "total_values(df_new,'Circumstances_of_Death',natural_causes,'Natural Causes')\n",
    "\n",
    "# Taking only motor vehicle accidents\n",
    "mva=['Motor Vehicle Accident','MVA']\n",
    "total_values(df_new,'Circumstances_of_Death',mva,'Motor Accident')\n",
    "\n",
    "# Taking only non-motor vehicle accidents\n",
    "non_mva=['Non-Motor Vehicle Accident','Accident, Non-MVA']\n",
    "total_values(df_new,'Circumstances_of_Death',non_mva,'Non-motor Accident')\n",
    "\n",
    "# Suicide - real or alleged\n",
    "suicide=['Suicide','Alleged Suicide']\n",
    "total_values(df_new,'Circumstances_of_Death',suicide,'Suicide')\n",
    "\n",
    "# Homicide - real or alleged\n",
    "homicide=['Homicide','Alleged Homicide']\n",
    "total_values(df_new,'Circumstances_of_Death',homicide,'Homicide')\n",
    "\n",
    "# Child Abuse - real or alleged\n",
    "child_abuse=['Child Abuse','Alleged Child Abuse']\n",
    "total_values(df_new,'Circumstances_of_Death',child_abuse,'Homicide')\n",
    "\n",
    "# Other/none of the above\n",
    "other=['Other','None of the Above']\n",
    "total_values(df_new,'Circumstances_of_Death',other,'Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50e94fb-8db6-4964-b46e-88cf9043af7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering: dealing with time\n",
    "def get_duration_between_dates(then, now, interval = \"default\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns a duration as specified by variable interval.\n",
    "    Used to calculate new feature of time authorized - time approached.\n",
    "\n",
    "    Code source: https://stackoverflow.com/questions/1345827/how-do-i-find-the-time-difference-between-two-datetime-objects-in-python\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    then (DateTime): a date-time.\n",
    "    now (DateTime): another date-time.\n",
    "    interval (string): type of duration metric, e.g. minutes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (float): A float with the duration in interval units.\n",
    "    \"\"\"\n",
    "\n",
    "    duration = now - then # For build-in functions\n",
    "    duration_in_s = duration.total_seconds()\n",
    "\n",
    "    def years():\n",
    "      return divmod(duration_in_s, 31536000) # Seconds in a year=31536000.\n",
    "\n",
    "    def days(seconds = None):\n",
    "      return divmod(seconds if seconds != None else duration_in_s, 86400) # Seconds in a day = 86400\n",
    "\n",
    "    def hours(seconds = None):\n",
    "      return divmod(seconds if seconds != None else duration_in_s, 3600) # Seconds in an hour = 3600\n",
    "\n",
    "    def minutes(seconds = None):\n",
    "      return divmod(seconds if seconds != None else duration_in_s, 60) # Seconds in a minute = 60\n",
    "\n",
    "    def seconds(seconds = None):\n",
    "      if seconds != None:\n",
    "        return divmod(seconds, 1)\n",
    "      return duration_in_s\n",
    "\n",
    "    def totalDuration():\n",
    "        y = years()\n",
    "        d = days(y[1]) # Use remainder to calculate next variable\n",
    "        h = hours(d[1])\n",
    "        m = minutes(h[1])\n",
    "        s = seconds(m[1])\n",
    "\n",
    "        return \"Time between dates: {} years, {} days, {} hours, {} minutes and {} seconds\".format(int(y[0]), int(d[0]), int(h[0]), int(m[0]), int(s[0]))\n",
    "\n",
    "    return {\n",
    "        'years': float(years()[0]),\n",
    "        'days': float(days()[0]),\n",
    "        'hours': float(hours()[0]),\n",
    "        'minutes': float(minutes()[0]),\n",
    "        'seconds': float(seconds()),\n",
    "        'default': totalDuration()\n",
    "    }\n",
    "\n",
    "\n",
    "def create_time_column(df,col1,col2,new_col_name):\n",
    "  \"\"\"\n",
    "  Create new column to describe the number of hours between administrative milestones,\n",
    "  e.g. between referral (death) and approach.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  df (DataFrame): input data.\n",
    "  col1 (string): name of column representing one timepoint.\n",
    "  col2 (string): name of column representing another timepoint.\n",
    "  new_col_name (string): new column name representing a time category between time points.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  df (DataFrame): modified df with new column.\n",
    "\n",
    "  \"\"\"\n",
    "  def convert_datetime(str1,str2):\n",
    "    # Helper function to convert to datetime\n",
    "    return [pd.to_datetime(str1), pd.to_datetime(str2)]\n",
    "\n",
    "  time_category = []\n",
    "  for row in zip(df[col1], df[col2]):\n",
    "    if pd.isnull(row[0])==False and pd.isnull(row[1])==False:\n",
    "      date_row=convert_datetime(row[0],row[1])\n",
    "      time_elapsed=abs(get_duration_between_dates(date_row[0],date_row[1])['hours'])\n",
    "\n",
    "      if time_elapsed <= 24:\n",
    "        time_category.append('Within 24 hours')\n",
    "\n",
    "      if time_elapsed > 24:\n",
    "        time_category.append('Over 24 hours')\n",
    "\n",
    "    else:\n",
    "      time_category.append('Milestone not reached')\n",
    "\n",
    "  df[new_col_name]=time_category\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "# Define timepoint variables\n",
    "time_vars = ['time_asystole','time_brain_death','time_referred', 'time_approached', 'time_authorized', 'time_procured']\n",
    "\n",
    "# Get category of intervals between them\n",
    "asystole_to_referred = 'time_asystole_to_referred'\n",
    "df_new = create_time_column(df_new,time_vars[0],time_vars[2], asystole_to_referred)\n",
    "\n",
    "brain_death_to_referred = 'time_brain_death_to_referred'\n",
    "df_new = create_time_column(df_new,time_vars[1],time_vars[2], brain_death_to_referred)\n",
    "\n",
    "referred_to_approached = 'time_referred_to_approached'\n",
    "df_new = create_time_column(df_new,time_vars[2],time_vars[3], referred_to_approached)\n",
    "\n",
    "approached_to_authorized = 'time_approached_to_authorized'\n",
    "df_new = create_time_column(df_new,time_vars[3],time_vars[4], approached_to_authorized)\n",
    "\n",
    "authorized_to_procured = 'time_authorized_to_procured'\n",
    "df_new = create_time_column(df_new,time_vars[4],time_vars[5], authorized_to_procured)\n",
    "\n",
    "\n",
    "def get_missing_data(data):\n",
    "  \"\"\"\n",
    "  Returns DataFrame with percent missing data from input data (DataFrame).\n",
    "\n",
    "  Parameters\n",
    "  -----\n",
    "  data (DataFrame): input dataframe\n",
    "\n",
    "  Returns\n",
    "  -----\n",
    "  missing_data (DataFrame): output dataframe with % missing values\n",
    "  \"\"\"\n",
    "\n",
    "  #print(data.isnull().sum()) # uncomment this if you want to see list of counts\n",
    "\n",
    "  # Get percentage of missing values in each column\n",
    "  missing_data_prop={}\n",
    "  for x,y in enumerate(list(data.isnull().sum())):\n",
    "    missing_data_prop[data.columns[x]]=(float(y/data.shape[0])*100) #\"{:.2f}\".format\n",
    "\n",
    "  missing_data=pd.DataFrame(missing_data_prop.items(), columns=['column', 'percent_missing'])\n",
    "  return missing_data\n",
    "\n",
    "missing_data=get_missing_data(df_new)\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a5008b-c49b-499e-abcb-b8b717bda1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_large_missing=list(missing_data[missing_data['percent_missing']>50]['column'])\n",
    "print(f'{len(cols_large_missing)} columns to drop due to over 50% missing')\n",
    "cols_large_missing #over 50% missing\n",
    "\n",
    "\n",
    "\n",
    "df_new2=df_new.copy()\n",
    "# Drop time variables and keep (some of) the time interval variables\n",
    "cols_large_missing.remove('Cause_of_Death_OPO') # Keep this as it is still domain relevant\n",
    "cols_large_missing.remove('time_brain_death')\n",
    "cols_large_missing.remove('time_approached')\n",
    "cols_large_missing.remove('time_authorized')\n",
    "df_new = df_new.drop(cols_large_missing,axis=1) # drop Procured_Year as it is almost perfectly collinear with Referral_Year (0.98)\n",
    "   \n",
    "\n",
    "# Make this copy before we remove collinear variables >0.8\n",
    "df_new_with_collinear=df_new.copy()\n",
    "cols_collinear = ['brain_death','time_referred','time_asystole','authorized','procured','time_approached_to_authorized','time_authorized_to_procured']\n",
    "df_new = df_new.drop(cols_collinear,axis=1)\n",
    "\n",
    "print(len(df_new.columns))\n",
    "df_new.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50a387f-7a65-4e3a-abd4-da47dfc19983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU 메모리 설정\n",
    "try:\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"===== MAE 기반 결측치 처리 및 ANN 모델 =====\")\n",
    "\n",
    "# 1. 기본 데이터 전처리\n",
    "df_processed = df_new.copy()\n",
    "\n",
    "# 불리언 변수를 정수형으로 변환\n",
    "for col in df_processed.columns:\n",
    "    if df_processed[col].dtype == 'bool':\n",
    "        df_processed[col] = df_processed[col].astype(int)\n",
    "\n",
    "# 시간 관련 변수 처리 (날짜/시간 변수를 수치형으로 변환)\n",
    "for col in df_processed.columns:\n",
    "    if pd.api.types.is_datetime64_any_dtype(df_processed[col]):\n",
    "        if not df_processed[col].isna().all():\n",
    "            reference_date = df_processed[col].min()\n",
    "            df_processed[col] = (df_processed[col] - reference_date).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "# 목표 변수 분리\n",
    "y = df_processed['transplanted'].values\n",
    "X_df = df_processed.drop('transplanted', axis=1)\n",
    "\n",
    "# 범주형 변수 인코딩\n",
    "categorical_cols = X_df.select_dtypes(include=['object', 'category']).columns\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    mask = X_df[col].notna()\n",
    "    if mask.sum() > 0:\n",
    "        X_df.loc[mask, col] = le.fit_transform(X_df.loc[mask, col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# 모든 열을 숫자형으로 변환\n",
    "X_numeric = X_df.apply(pd.to_numeric, errors='coerce').values\n",
    "\n",
    "print(f\"데이터 형태: X={X_numeric.shape}, y={y.shape}\")\n",
    "print(f\"전체 결측치 개수: {np.isnan(X_numeric).sum()}\")\n",
    "print(f\"결측치 비율: {np.isnan(X_numeric).sum() / X_numeric.size * 100:.2f}%\")\n",
    "\n",
    "# 2. 데이터 분할 (결측치 유지)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_numeric, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\n데이터 분할 완료:\")\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# 3. 개선된 Masked AutoEncoder 클래스\n",
    "class ImprovedMaskedAutoEncoder:\n",
    "    def __init__(self, input_dim, hidden_dims=[256, 128, 64], dropout_rate=0.3, noise_factor=0.1):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.noise_factor = noise_factor\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def build_model(self):\n",
    "        # 입력과 마스크\n",
    "        input_layer = layers.Input(shape=(self.input_dim,), name='input')\n",
    "        mask_layer = layers.Input(shape=(self.input_dim,), name='mask')\n",
    "        \n",
    "        # 노이즈 추가 (더 강건한 학습을 위해)\n",
    "        noise = layers.GaussianNoise(self.noise_factor)(input_layer)\n",
    "        \n",
    "        # 마스킹 적용\n",
    "        masked_input = layers.Multiply(name='masked_input')([noise, mask_layer])\n",
    "        \n",
    "        # 인코더 (더 깊고 복잡하게)\n",
    "        encoded = masked_input\n",
    "        for i, hidden_dim in enumerate(self.hidden_dims):\n",
    "            encoded = layers.Dense(hidden_dim, activation='relu', name=f'encoder_{i}')(encoded)\n",
    "            encoded = layers.BatchNormalization(name=f'bn_encoder_{i}')(encoded)\n",
    "            encoded = layers.Dropout(self.dropout_rate, name=f'dropout_encoder_{i}')(encoded)\n",
    "        \n",
    "        # 디코더\n",
    "        decoded = encoded\n",
    "        for i, hidden_dim in enumerate(reversed(self.hidden_dims[:-1])):\n",
    "            decoded = layers.Dense(hidden_dim, activation='relu', name=f'decoder_{i}')(decoded)\n",
    "            decoded = layers.BatchNormalization(name=f'bn_decoder_{i}')(decoded)\n",
    "            decoded = layers.Dropout(self.dropout_rate, name=f'dropout_decoder_{i}')(decoded)\n",
    "        \n",
    "        # 출력층\n",
    "        output = layers.Dense(self.input_dim, activation='linear', name='output')(decoded)\n",
    "        \n",
    "        # 모델 생성\n",
    "        self.model = keras.Model([input_layer, mask_layer], output, name='MaskedAutoEncoder')\n",
    "        \n",
    "        # 컴파일 (더 나은 최적화를 위해)\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "            loss='huber',  # MSE보다 이상치에 강건\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "    def create_mask(self, X):\n",
    "        \"\"\"결측치 마스크 생성 (관측된 값은 1, 결측치는 0)\"\"\"\n",
    "        return (~np.isnan(X)).astype(np.float32)\n",
    "    \n",
    "    def prepare_training_data(self, X):\n",
    "        \"\"\"학습용 데이터 준비\"\"\"\n",
    "        # 결측치를 0으로 대체 (임시)\n",
    "        X_filled = np.nan_to_num(X, nan=0.0)\n",
    "        \n",
    "        # 표준화\n",
    "        X_scaled = self.scaler.fit_transform(X_filled)\n",
    "        \n",
    "        # 마스크 생성\n",
    "        mask = self.create_mask(X)\n",
    "        \n",
    "        return X_scaled.astype(np.float32), mask.astype(np.float32)\n",
    "    \n",
    "    def prepare_inference_data(self, X):\n",
    "        \"\"\"추론용 데이터 준비\"\"\"\n",
    "        X_filled = np.nan_to_num(X, nan=0.0)\n",
    "        X_scaled = self.scaler.transform(X_filled)\n",
    "        mask = self.create_mask(X)\n",
    "        \n",
    "        return X_scaled.astype(np.float32), mask.astype(np.float32)\n",
    "    \n",
    "    def fit(self, X, epochs=100, batch_size=64, validation_split=0.15, verbose=1):\n",
    "        \"\"\"MAE 모델 학습\"\"\"\n",
    "        if self.model is None:\n",
    "            self.build_model()\n",
    "        \n",
    "        X_scaled, mask = self.prepare_training_data(X)\n",
    "        \n",
    "        # 콜백 설정\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=15,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.7,\n",
    "                patience=7,\n",
    "                min_lr=1e-6,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        print(\"MAE 모델 학습 시작...\")\n",
    "        history = self.model.fit(\n",
    "            [X_scaled, mask], X_scaled,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=callbacks,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def impute(self, X):\n",
    "        \"\"\"결측치 보간\"\"\"\n",
    "        X_scaled, mask = self.prepare_inference_data(X)\n",
    "        \n",
    "        # 모델 예측\n",
    "        reconstructed = self.model.predict([X_scaled, mask], verbose=0)\n",
    "        \n",
    "        # 원본 스케일로 복원\n",
    "        reconstructed_original = self.scaler.inverse_transform(reconstructed)\n",
    "        \n",
    "        # 결측치만 보간값으로 대체\n",
    "        X_imputed = X.copy()\n",
    "        missing_mask = np.isnan(X)\n",
    "        X_imputed[missing_mask] = reconstructed_original[missing_mask]\n",
    "        \n",
    "        return X_imputed\n",
    "\n",
    "# 4. MAE 모델 학습 및 결측치 보간\n",
    "print(\"\\n===== MAE 모델 학습 =====\")\n",
    "mae = ImprovedMaskedAutoEncoder(\n",
    "    input_dim=X_train.shape[1],\n",
    "    hidden_dims=[512, 256, 128, 64],  # 더 깊은 네트워크\n",
    "    dropout_rate=0.3,\n",
    "    noise_factor=0.05\n",
    ")\n",
    "\n",
    "# MAE 학습\n",
    "mae_history = mae.fit(X_train, epochs=80, batch_size=32, verbose=1)\n",
    "\n",
    "# 결측치 보간\n",
    "print(\"\\n결측치 보간 수행 중...\")\n",
    "X_train_imputed = mae.impute(X_train)\n",
    "X_val_imputed = mae.impute(X_val)\n",
    "X_test_imputed = mae.impute(X_test)\n",
    "\n",
    "# 보간 결과 확인\n",
    "print(f\"보간 후 결측치 개수:\")\n",
    "print(f\"X_train: {np.isnan(X_train_imputed).sum()}\")\n",
    "print(f\"X_val: {np.isnan(X_val_imputed).sum()}\")\n",
    "print(f\"X_test: {np.isnan(X_test_imputed).sum()}\")\n",
    "\n",
    "# 5. 최종 데이터 표준화\n",
    "final_scaler = StandardScaler()\n",
    "X_train_final = final_scaler.fit_transform(X_train_imputed)\n",
    "X_val_final = final_scaler.transform(X_val_imputed)\n",
    "X_test_final = final_scaler.transform(X_test_imputed)\n",
    "\n",
    "# 6. 클래스 가중치 계산\n",
    "pos_samples = np.sum(y_train)\n",
    "neg_samples = len(y_train) - pos_samples\n",
    "total_samples = len(y_train)\n",
    "\n",
    "weight_for_0 = (1 / neg_samples) * (total_samples / 2.0)\n",
    "weight_for_1 = (1 / pos_samples) * (total_samples / 2.0)\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print(f\"\\n클래스 분포:\")\n",
    "print(f\"Class 0 (이식되지 않음): {neg_samples} ({neg_samples/total_samples*100:.2f}%)\")\n",
    "print(f\"Class 1 (이식됨): {pos_samples} ({pos_samples/total_samples*100:.2f}%)\")\n",
    "print(f\"클래스 가중치: {class_weight}\")\n",
    "\n",
    "# 7. 개선된 ANN 모델\n",
    "def create_improved_ann_model(input_dim):\n",
    "    \"\"\"개선된 ANN 모델 생성\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # 첫 번째 블록\n",
    "        keras.layers.Dense(256, activation='relu', input_dim=input_dim),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.4),\n",
    "        \n",
    "        # 두 번째 블록\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        \n",
    "        # 세 번째 블록\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        \n",
    "        # 네 번째 블록\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        \n",
    "        # 출력층\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def apply_improved_ann_model(X_train, y_train, X_val, y_val, X_test, y_test, class_weight):\n",
    "    \"\"\"개선된 ANN 모델 학습 및 평가\"\"\"\n",
    "    print(\"\\n===== ANN 모델 학습 및 평가 =====\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 모델 생성\n",
    "    model = create_improved_ann_model(X_train.shape[1])\n",
    "    \n",
    "    # 컴파일\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    # 콜백 설정\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=10,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # 모델 학습\n",
    "    print(\"ANN 모델 학습 중...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=150,\n",
    "        batch_size=32,\n",
    "        class_weight=class_weight,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # 예측\n",
    "    y_val_prob = model.predict(X_val, verbose=0).flatten()\n",
    "    y_test_prob = model.predict(X_test, verbose=0).flatten()\n",
    "    \n",
    "    # 임계값 최적화 (F1 점수 기준)\n",
    "    thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_val_pred_temp = (y_val_prob >= threshold).astype(int)\n",
    "        f1_temp = f1_score(y_val, y_val_pred_temp, zero_division=0)\n",
    "        if f1_temp > best_f1:\n",
    "            best_f1 = f1_temp\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    print(f\"최적 임계값: {best_threshold:.3f} (F1: {best_f1:.4f})\")\n",
    "    \n",
    "    # 최적 임계값으로 예측\n",
    "    y_val_pred = (y_val_prob >= best_threshold).astype(int)\n",
    "    y_test_pred = (y_test_prob >= best_threshold).astype(int)\n",
    "    \n",
    "    # 성능 평가\n",
    "    def evaluate_performance(y_true, y_pred, y_prob, set_name):\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "        \n",
    "        # 민감도 (Sensitivity) = 재현율 (Recall)\n",
    "        sensitivity = recall\n",
    "        \n",
    "        # 특이도 (Specificity)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        \n",
    "        print(f\"\\n===== {set_name} Set 성능 =====\")\n",
    "        print(f\"정확도 (Accuracy): {accuracy:.4f}\")\n",
    "        print(f\"정밀도 (Precision): {precision:.4f}\")\n",
    "        print(f\"재현율/민감도 (Recall/Sensitivity): {sensitivity:.4f}\")\n",
    "        print(f\"특이도 (Specificity): {specificity:.4f}\")\n",
    "        print(f\"F1 점수: {f1:.4f}\")\n",
    "        print(f\"AUC: {auc:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'sensitivity': sensitivity,\n",
    "            'specificity': specificity,\n",
    "            'f1': f1,\n",
    "            'auc': auc,\n",
    "            'confusion_matrix': confusion_matrix(y_true, y_pred)\n",
    "        }\n",
    "    \n",
    "    # 성능 평가\n",
    "    val_results = evaluate_performance(y_val, y_val_pred, y_val_prob, \"Validation\")\n",
    "    test_results = evaluate_performance(y_test, y_test_pred, y_test_prob, \"Test\")\n",
    "    \n",
    "    # 실행 시간\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    # 시각화\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # 학습 과정\n",
    "    plt.subplot(2, 4, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 4, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 혼동 행렬\n",
    "    plt.subplot(2, 4, 3)\n",
    "    sns.heatmap(val_results['confusion_matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Validation Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    \n",
    "    plt.subplot(2, 4, 4)\n",
    "    sns.heatmap(test_results['confusion_matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Test Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    \n",
    "    # ROC 곡선\n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr_val, tpr_val, _ = roc_curve(y_val, y_val_prob)\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_test, y_test_prob)\n",
    "    \n",
    "    plt.subplot(2, 4, 5)\n",
    "    plt.plot(fpr_val, tpr_val, label=f'Validation ROC (AUC = {val_results[\"auc\"]:.3f})')\n",
    "    plt.plot(fpr_test, tpr_test, label=f'Test ROC (AUC = {test_results[\"auc\"]:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 성능 지표 비교\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
    "    val_scores = [val_results[m] for m in metrics]\n",
    "    test_scores = [test_results[m] for m in metrics]\n",
    "    \n",
    "    plt.subplot(2, 4, 6)\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    plt.bar(x - width/2, val_scores, width, label='Validation', alpha=0.8)\n",
    "    plt.bar(x + width/2, test_scores, width, label='Test', alpha=0.8)\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Performance Comparison')\n",
    "    plt.xticks(x, metrics, rotation=45)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 결과 반환\n",
    "    results = {\n",
    "        'model': model,\n",
    "        'history': history.history,\n",
    "        'validation': val_results,\n",
    "        'test': test_results,\n",
    "        'execution_time': execution_time,\n",
    "        'best_threshold': best_threshold\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 8. 모델 학습 및 평가\n",
    "results = apply_improved_ann_model(\n",
    "    X_train_final, y_train, \n",
    "    X_val_final, y_val, \n",
    "    X_test_final, y_test, \n",
    "    class_weight\n",
    ")\n",
    "\n",
    "# 9. 최종 결과 요약\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"최종 결과 요약 - MAE + 개선된 ANN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"테스트 세트 성능 (최적 임계값: {results['best_threshold']:.3f}):\")\n",
    "print(f\"- 정확도: {results['test']['accuracy']:.4f}\")\n",
    "print(f\"- 정밀도: {results['test']['precision']:.4f}\")\n",
    "print(f\"- 재현율/민감도: {results['test']['sensitivity']:.4f}\")\n",
    "print(f\"- 특이도: {results['test']['specificity']:.4f}\")\n",
    "print(f\"- F1 점수: {results['test']['f1']:.4f}\")\n",
    "print(f\"- AUC: {results['test']['auc']:.4f}\")\n",
    "print(f\"- 총 실행 시간: {results['execution_time']:.2f}초\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# MAE 효과 시각화\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(mae_history.history['loss'], label='Training Loss')\n",
    "plt.plot(mae_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('MAE Training Process')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# 보간 전후 데이터 분포 비교 (첫 번째 특성 기준)\n",
    "plt.hist(X_train[~np.isnan(X_train[:, 0]), 0], bins=30, alpha=0.5, label='Original (non-missing)', density=True)\n",
    "plt.hist(X_train_imputed[:, 0], bins=30, alpha=0.5, label='After MAE imputation', density=True)\n",
    "plt.title('Data Distribution Before/After MAE')\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n논문 작성을 위한 핵심 포인트:\")\n",
    "print(\"1. MAE를 통한 지능적 결측치 보간으로 데이터 품질 향상\")\n",
    "print(\"2. 클래스 불균형 문제 해결을 위한 가중치 적용\")\n",
    "print(\"3. 최적 임계값 탐색을 통한 F1 점수 최적화\")\n",
    "print(\"4. 배치 정규화와 드롭아웃을 통한 과적합 방지\")\n",
    "print(\"5. 조기 종료와 학습률 감소를 통한 안정적 학습\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e7d11-e5ba-4f88-b1ed-24985443f351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
